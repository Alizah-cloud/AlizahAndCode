{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Alizah-cloud/AlizahAndCode/blob/main/Sentimental_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HKKJNjZC-Mx"
   },
   "outputs": [],
   "source": [
    "import contractions\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8Gi0JCCC-M0"
   },
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9XP_aXcC-M5"
   },
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HC3ZnOdGC-M6"
   },
   "outputs": [],
   "source": [
    "def detect_contractions(text):\n",
    "    detected_contractions = [word for word in str(text).split() if \"'\" in word]\n",
    "    return detected_contractions\n",
    "\n",
    "reviews['contractions'] = reviews['review'].apply(detect_contractions)\n",
    "\n",
    "def expand_contractions_in_column(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "reviews[\"Expanded_reviews\"] = reviews[\"review\"].apply(expand_contractions_in_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  \n",
    "    text = text.lower()  \n",
    "    \n",
    "    \n",
    "    words = text.split()\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    words = [word for word in words if word not in stop_words]\n",
    "  \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    preprocessed_text = \" \".join(words)\n",
    "    return preprocessed_text\n",
    "\n",
    "\n",
    "reviews['preprocessed_reviews'] = reviews['Expanded_reviews'].apply(preprocess_text)\n",
    "print(reviews[['Expanded_reviews', 'preprocessed_reviews']].sample(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PyDPCDuFC-M_",
    "outputId": "ddbde0a9-75e1-49dc-d157-677965027a28"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "reviews['sentiment'] = label_encoder.fit_transform(reviews['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews['preprocessed_reviews'] \n",
    "y = reviews['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tK3Ixc0yC-ND"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "xFlO7pCggT2L",
    "outputId": "528db9d2-f6e6-418a-cb3b-062b44431a01"
   },
   "outputs": [],
   "source": [
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "maxlen = 200  # Maximum length of a review\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=maxlen, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=maxlen, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "de3Eizi0C-NF"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=20000, output_dim=128, input_length=maxlen),\n",
    "    LSTM(64, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "YvY1s8QoC-NF"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_padded, y_train, validation_data=(X_test_padded, y_test), epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Rr25OQyph846"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PvdNshQIiApm"
   },
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save('sentiment_analysis_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer to a file\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
